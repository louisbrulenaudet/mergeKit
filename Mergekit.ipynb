{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Large Language Models MergeKit\n",
        "\n",
        "[![Python](https://img.shields.io/pypi/pyversions/tensorflow.svg)](https://badge.fury.io/py/tensorflow)\n",
        "![Maintainer](https://img.shields.io/badge/maintainer-@louisbrulenaudet-blue)\n",
        "\n",
        "Mergekit uses an out-of-core approach to perform unreasonably elaborate merges in resource-constrained situations. Merges can be run entirely on CPU or accelerated with as little as 8 GB of VRAM. Many merging algorithms are supported, with more coming as they catch my attention.\n",
        "\n",
        "When you have a merged model you're happy with, you may want to share it on the Hugging Face Hub. mergekit generates a README.md for your merge with some basic information for a model card. You can edit it to include more details about your merge, like giving it a good name or explaining what it's good at; rewrite it entirely ; or use the generated README.md as-is. It is also possible to edit your README.md online once it has been uploaded to the Hub.\n",
        "\n",
        "## Citing this project\n",
        "\n",
        "If you use this code in your research, please use the following BibTeX entry.\n",
        "\n",
        "```BibTeX\n",
        "@misc{louisbrulenaudet2023,\n",
        "  author =       {Louis BrulÃ© Naudet},\n",
        "  title =        {Large Language Models MergeKit},\n",
        "  year =         {2023}\n",
        "}\n",
        "```\n",
        "\n",
        "## Feedback\n",
        "\n",
        "If you have any feedback, please reach out at [louisbrulenaudet@icloud.com](mailto:louisbrulenaudet@icloud.com).\n"
      ],
      "metadata": {
        "id": "1Wq4SB9A_9ic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge Configuration\n",
        "\n",
        "Merge configurations are YAML documents specifying the operations to perform in order to produce your merged model.\n",
        "Below are the primary elements of a configuration file:\n",
        "\n",
        "- `merge_method`: Specifies the method to use for merging models. See [Merge Methods](#merge-methods) for a list.\n",
        "- `slices`: Defines slices of layers from different models to be used. This field is mutually exclusive with `models`.\n",
        "- `models`: Defines entire models to be used for merging. This field is mutually exclusive with `slices`.\n",
        "- `base_model`: Specifies the base model used in some merging methods.\n",
        "- `parameters`: Holds various parameters such as weights and densities, which can also be specified at different levels of the configuration.\n",
        "- `dtype`: Specifies the data type used for the merging operation.\n",
        "- `tokenizer_source`: Determines how to construct a tokenizer for the merged model.\n",
        "\n",
        "## Merge Methods\n",
        "\n",
        "### Spherical Linear Interpolation\n",
        "\n",
        "Spherical Linear Interpolation (SLERP) serves as a technique for seamlessly interpolating between two vectors while maintaining a constant rate of change and upholding the geometric properties of the spherical space in which these vectors exist.\n",
        "\n",
        "Opting for SLERP over traditional linear interpolation is motivated by various considerations. Linear interpolation in high-dimensional spaces may result in a reduction in the magnitude of the interpolated vector, diminishing the scale of weights. Additionally, in many cases, the alteration in the weights' direction conveys more meaningful information, such as feature learning and representation, compared to the magnitude of change.\n",
        "\n",
        "The implementation of SLERP involves the following steps:\n",
        "\n",
        "- Normalize the input vectors to unit length, ensuring they signify directions rather than magnitudes.\n",
        "- Calculate the angle between these vectors using their dot product.\n",
        "- If the vectors are nearly collinear, the method defaults to linear interpolation for efficiency. Otherwise, SLERP calculates scale factors based on the interpolation factor t (where t=0 corresponds to 100% of the first vector, and t=1 corresponds to 100% of the second vector) and the angle between the vectors.\n",
        "- Utilize these computed factors to weigh the original vectors, and then sum them to derive the interpolated vector.\n",
        "\n",
        "### Ties merging\n",
        "\n",
        "TIES-Merging is a method designed to facilitate the efficient merging of multiple task-specific models into a consolidated multitask model. It addresses two primary challenges encountered in the process of model merging with a focus on maintaining objectivity.\n",
        "\n",
        "One key challenge tackled by TIES-Merging involves addressing redundancy in model parameters. This is achieved by identifying and eliminating redundant parameters within task-specific models, emphasizing the changes made during fine-tuning and selectively retaining the top-k% most significant changes while discarding the rest.\n",
        "\n",
        "Another challenge pertains to conflicts arising from disagreements between parameter signs across different models. TIES-Merging resolves these conflicts by creating a unified sign vector representing the most dominant direction of change across all models.\n",
        "\n",
        "The TIES-Merging process consists of three steps:\n",
        "\n",
        "- Trim: Reduces redundancy in task-specific models by retaining a fraction of the most significant parameters (density parameter) and resetting the remaining parameters to zero.\n",
        "- Elect Sign: Resolves sign conflicts across different models by creating a unified sign vector based on the most dominant direction (positive or negative) in terms of cumulative magnitude.\n",
        "- Disjoint Merge: Averages parameter values aligned with the unified sign vector, excluding zero values."
      ],
      "metadata": {
        "id": "Y2CwHeZm37Lz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face login"
      ],
      "metadata": {
        "id": "DUffJmmpWgzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global credential.helper store\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "D6OE_f3Kn5Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model configuration"
      ],
      "metadata": {
        "id": "ugC3CcYTWubM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Pearl-7B-0212-ties\"\n",
        "yaml_config = \"\"\"\n",
        "models:\n",
        "  - model: OpenPipe/mistral-ft-optimized-1227\n",
        "  - model: louisbrulenaudet/Pearl-7B-slerp\n",
        "    parameters:\n",
        "      density: 0.6\n",
        "      weight: 0.3\n",
        "  - model: WizardLM/WizardMath-7B-V1.1\n",
        "    parameters:\n",
        "      density: 0.55\n",
        "      weight: 0.2\n",
        "  - model: macadeliccc/WestLake-7B-v2-laser-truthy-dpo\n",
        "    parameters:\n",
        "      density: 0.55\n",
        "      weight: 0.25\n",
        "  - model: CultriX/NeuralTrix-7B-dpo\n",
        "    parameters:\n",
        "     density: 0.6\n",
        "     weight: 0.25\n",
        "merge_method: ties\n",
        "base_model: OpenPipe/mistral-ft-optimized-1227\n",
        "parameters:\n",
        "  normalize: true\n",
        "  int8_mask: true\n",
        "dtype: float16\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LGd7jlfCpNcg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_merging(runtime: str = \"CPU\", branch: str = \"main\", trust_remote_code: bool = False) -> None:\n",
        "    \"\"\"\n",
        "    Merges models using Mergekit with specified configurations.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    runtime : str, optional\n",
        "        Selects the runtime type for merging. Default is \"CPU\".\n",
        "\n",
        "    branch : str, optional\n",
        "        Selects the branch to use for Mergekit. Default is \"main\".\n",
        "\n",
        "    trust_remote_code : bool, optional\n",
        "        Specifies whether to trust remote code during merging. Default is False.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        This function does not return anything. It merges models according to the specified configurations.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        !git clone https://github.com/louisbrulenaudet/mergekit.git\n",
        "\n",
        "        if branch == \"main\":\n",
        "            !git clone https://github.com/cg123/mergekit.git\n",
        "            !cd mergekit && pip install -qqq -e . --progress-bar off\n",
        "\n",
        "        elif branch == \"mixtral\":\n",
        "            !git clone -b mixtral https://github.com/cg123/mergekit.git\n",
        "            !cd mergekit && pip install -qqq -e . --progress-bar off\n",
        "            !pip install -qqq -U transformers --progress-bar off\n",
        "\n",
        "        # Save config as yaml file\n",
        "        with open(\"config.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(yaml_config)\n",
        "\n",
        "        # Base CLI\n",
        "        if branch == \"main\":\n",
        "            cli = \"mergekit-yaml config.yaml merge --copy-tokenizer\"\n",
        "\n",
        "        elif branch == \"mixtral\":\n",
        "            cli = \"mergekit-moe config.yaml merge --copy-tokenizer\"\n",
        "\n",
        "        # Additional arguments\n",
        "        if runtime == \"CPU\":\n",
        "            cli += \" --allow-crimes --out-shard-size 1B --lazy-unpickle\"\n",
        "\n",
        "        elif runtime == \"GPU\":\n",
        "\n",
        "            cli += \" --cuda --low-cpu-memory\"\n",
        "        if trust_remote_code:\n",
        "\n",
        "            cli += \" --trust-remote-code\"\n",
        "\n",
        "        print(cli)\n",
        "\n",
        "        # Merge models\n",
        "        !{cli}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred during model merging:\", e)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "# @title ## Model merging\n",
        "\n",
        "# @markdown ### Runtime type\n",
        "# @markdown Select your runtime (CPU, High RAM, GPU)\n",
        "\n",
        "runtime = \"CPU\" # @param [\"CPU\", \"CPU + High-RAM\", \"GPU\"]\n",
        "\n",
        "# @markdown ### Mergekit arguments\n",
        "# @markdown Use the `main` branch by default, [`mixtral`](https://github.com/cg123/mergekit/blob/mixtral/moe.md) if you want to create a Mixture of Experts.\n",
        "\n",
        "branch = \"main\" # @param [\"main\", \"mixtral\"]\n",
        "trust_remote_code = False # @param {type:\"boolean\"}\n",
        "\n",
        "model_merging(\n",
        "    runtime=runtime,\n",
        "    branch=branch,\n",
        "    trust_remote_code=trust_remote_code\n",
        ")"
      ],
      "metadata": {
        "id": "d5mYzDo1q96y",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU huggingface_hub\n",
        "import yaml\n",
        "from huggingface_hub import ModelCard, HfApi\n",
        "from google.colab import userdata\n",
        "from jinja2 import Template\n",
        "\n",
        "main_template = \"\"\"\n",
        "---\n",
        "license: {{ license }}\n",
        "language:\n",
        "- {{ language }}\n",
        "library_name: transformers\n",
        "base_model:\n",
        "{%- for model in models %}\n",
        "- {{ model }}\n",
        "{%- endfor %}\n",
        "tags:\n",
        "- merge\n",
        "- mergekit\n",
        "{%- for model in models %}\n",
        "- {{ model }}\n",
        "{%- endfor %}\n",
        "---\n",
        "\n",
        "# {{ model_name }}\n",
        "\n",
        "{{ model_name }} is a merge of the following models :\n",
        "\n",
        "{%- for model in models %}\n",
        "* [{{ model }}](https://huggingface.co/{{ model }})\n",
        "{%- endfor %}\n",
        "\n",
        "## Configuration\n",
        "\n",
        "```yaml\n",
        "{{- yaml_config -}}\n",
        "```\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "!pip install -qU transformers accelerate\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model = \"{{ username }}/{{ model_name }}\"\n",
        "messages = [{\"role\": \"user\", \"content\": \"What is a large language model?\"}]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "print(outputs[0][\"generated_text\"])\n",
        "```\n",
        "## Feedback\n",
        "\n",
        "If you have any feedback, please reach out at [louisbrulenaudet@icloud.com](mailto:louisbrulenaudet@icloud.com).\n",
        "\"\"\"\n",
        "\n",
        "moe_template = \"\"\"\n",
        "---\n",
        "license: {{ license }}\n",
        "language:\n",
        "- {{ language }}\n",
        "library_name: transformers\n",
        "base_model:\n",
        "{%- for model in models %}\n",
        "  - {{ model }}\n",
        "{%- endfor %}\n",
        "tags:\n",
        "- moe\n",
        "- merge\n",
        "- mergekit\n",
        "{%- for model in models %}\n",
        "- {{ model }}\n",
        "{%- endfor %}\n",
        "---\n",
        "\n",
        "# {{ model_name }}\n",
        "\n",
        "{{ model_name }} is a Mixure of Experts (MoE) made with the following models :\n",
        "\n",
        "{%- for model in models %}\n",
        "* [{{ model }}](https://huggingface.co/{{ model }})\n",
        "{%- endfor %}\n",
        "\n",
        "## Configuration\n",
        "\n",
        "```yaml\n",
        "{{- yaml_config -}}\n",
        "```\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "!pip install -qU transformers bitsandbytes accelerate\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model = \"{{ username }}/{{ model_name }}\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    model_kwargs={\"torch_dtype\": torch.float16, \"load_in_4bit\": True},\n",
        ")\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": \"Explain what a Mixture of Experts is in less than 100 words.\"}]\n",
        "prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "print(outputs[0][\"generated_text\"])\n",
        "```\n",
        "## Feedback\n",
        "\n",
        "If you have any feedback, please reach out at [louisbrulenaudet@icloud.com](mailto:louisbrulenaudet@icloud.com).\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Card:\n",
        "    def __init__(self, template_text:str, yaml_config:str, model_name:str, username:str, license:str) -> None:\n",
        "        \"\"\"\n",
        "        Initialize a Card object.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        template_text : str\n",
        "            The template text for creating the model card.\n",
        "\n",
        "        yaml_config : str\n",
        "            The YAML configuration containing model information.\n",
        "\n",
        "        model_name : str\n",
        "            The name of the model.\n",
        "\n",
        "        username : str\n",
        "            The Hugging Face username.\n",
        "\n",
        "        license : str\n",
        "            The license for the model.\n",
        "\n",
        "        path : str, optional\n",
        "            The path to save the model files. Default is \"merge\".\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "        self.template_text = template_text\n",
        "        self.yaml_config = yaml_config\n",
        "        self.model_name = model_name\n",
        "        self.username = username\n",
        "        self.license = license\n",
        "        self.path = path\n",
        "\n",
        "\n",
        "    def create(self, models:list) -> None:\n",
        "        \"\"\"\n",
        "        Create a model card.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        models : list\n",
        "            List of models to merge.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "            No return value.\n",
        "        \"\"\"\n",
        "        jinja_template = Template(self.template_text.strip())\n",
        "\n",
        "        content = jinja_template.render(\n",
        "            model_name=self.model_name,\n",
        "            models=models,\n",
        "            yaml_config=self.yaml_config,\n",
        "            username=self.username,\n",
        "            license=self.license\n",
        "        )\n",
        "\n",
        "        card = ModelCard(content)\n",
        "        card.save(\"merge/README.md\")\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "    def upload(self, path:str=\"./merge\") -> None:\n",
        "        \"\"\"\n",
        "        Upload the model to Hugging Face.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        path : str, optional\n",
        "            The path to the folder containing model files. Default is \"./merge\".\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "            No return value.\n",
        "        \"\"\"\n",
        "        api = HfApi(\n",
        "            token=userdata.get(token)\n",
        "        )\n",
        "\n",
        "        api.create_repo(\n",
        "            repo_id=f\"{self.username}/{self.model_name}\",\n",
        "            repo_type=\"model\",\n",
        "            exist_ok=True\n",
        "        )\n",
        "\n",
        "        api.upload_folder(\n",
        "            repo_id=f\"{self.username}/{self.model_name}\",\n",
        "            folder_path=self.path\n",
        "        )\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "class Main(Card):\n",
        "    def __init__(self, template_text:str, yaml_config:str, model_name:str, username:str, license:str):\n",
        "        \"\"\"\n",
        "        Initialize a Main object.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        template_text : str\n",
        "            The template text for creating the model card.\n",
        "\n",
        "        yaml_config : str\n",
        "            The YAML configuration containing model information.\n",
        "\n",
        "        model_name : str\n",
        "            The name of the model.\n",
        "\n",
        "        username : str\n",
        "            The Hugging Face username.\n",
        "\n",
        "        license : str\n",
        "            The license for the model.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "        super().__init__(\n",
        "            template_text=template_text,\n",
        "            yaml_config=yaml_config,\n",
        "            model_name=model_name,\n",
        "            username=username,\n",
        "            license=license\n",
        "        )\n",
        "\n",
        "\n",
        "    def create_card(self) -> None:\n",
        "        \"\"\"\n",
        "        Create a model card and upload it to Hugging Face.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        None\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "            No return value.\n",
        "        \"\"\"\n",
        "        data = yaml.safe_load(\n",
        "            self.yaml_config\n",
        "        )\n",
        "\n",
        "        models = [model[\"model\"] for model in data.get(\"models\", [])]\n",
        "\n",
        "        super().create(\n",
        "            models=models\n",
        "        )\n",
        "\n",
        "        super().upload(\n",
        "            path=\"./merge\"\n",
        "        )\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "class Moe(Card):\n",
        "    def __init__(self, template_text:str, yaml_config:str, model_name:str, username:str, license:str) -> None:\n",
        "        \"\"\"\n",
        "        Initialize a Moe object.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        template_text : str\n",
        "            The template text for creating the model card.\n",
        "\n",
        "        yaml_config : str\n",
        "            The YAML configuration containing model information.\n",
        "\n",
        "        model_name : str\n",
        "            The name of the model.\n",
        "\n",
        "        username : str\n",
        "            The Hugging Face username.\n",
        "\n",
        "        license : str\n",
        "            The license for the model.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "        super().__init__(\n",
        "            template_text=template_text,\n",
        "            yaml_config=yaml_config,\n",
        "            model_name=model_name,\n",
        "            username=username,\n",
        "            license=license\n",
        "        )\n",
        "\n",
        "\n",
        "    def create_card(self) -> None:\n",
        "        \"\"\"\n",
        "        Create a MoE model card and upload it to Hugging Face.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        None\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "            No return value.\n",
        "        \"\"\"\n",
        "        data = yaml.safe_load(\n",
        "            self.yaml_config\n",
        "        )\n",
        "\n",
        "        models = [model[\"source_model\"] for model in data.get(\"experts\", [])]\n",
        "\n",
        "        super().create(\n",
        "            models=models\n",
        "        )\n",
        "\n",
        "        super().upload(\n",
        "            path=\"./merge\"\n",
        "        )\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "# @title ## Upload model to Hugging Face { display-mode: \"form\" }\n",
        "# @markdown Enter your HF username and the name of Colab secret that stores your [Hugging Face access token](https://huggingface.co/settings/tokens).\n",
        "username = \"louisbrulenaudet\" # @param {type:\"string\"}\n",
        "token = \"hf\" # @param {type:\"string\"}\n",
        "license = \"apache-2.0\" # @param [\"apache-2.0\", \"cc-by-nc-4.0\", \"mit\", \"openrail\"] {allow-input: true}\n",
        "language = \"en\" # @param {type:\"string\"}\n",
        "\n",
        "if branch == \"main\":\n",
        "    instance = Main(\n",
        "        template_text=main_template,\n",
        "        yaml_config=yaml_config,\n",
        "        model_name=model_name,\n",
        "        username=username,\n",
        "        license=license\n",
        "    )\n",
        "\n",
        "elif branch == \"mixtral\":\n",
        "    instance = Moe(\n",
        "        template_text=moe_template,\n",
        "        yaml_config=yaml_config,\n",
        "        model_name=model_name,\n",
        "        username=username,\n",
        "        license=license\n",
        "    )\n",
        "\n",
        "instance.create_card()"
      ],
      "metadata": {
        "id": "ik0V0dF55gfU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}